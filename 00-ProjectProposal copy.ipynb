{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "Omar Abbasi: Project Administration, Conceptualization, Formal Analysis, Visualization, Writing – Original Draft  \n",
    "Zahir Ali: Conceptualization, Visualization, Software, Writing – Reviewing/Edits  \n",
    "Adam Hamadene: Research, Formal Analysis  \n",
    "Mostafa Darwish: Visualization, Writing – Original Draft, Data Curation  \n",
    "Yasir Rizvi: Data Cleaning \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do funding size, industry sector, and geographic location influence both the likelihood and timing of startup failure versus acquisition?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the world continues to see the intersection of human ingenuity and technological accumulation grow extremely rapidly, this shift in the corporate landscape can be tied back to a specific niche: the prevalence and growth of startups in the post-modern era. As technical knowledge and tools continue to develop, human ingenuity has found itself employed in finding the most useful ways to leverage and expand upon the current era of technology and artificial intelligence. Examples of post-modern startups include Uber, Robinhood, Stripe, Databricks, Canva, and Slack. Ideas for growth and innovation stem from all fields, and are catalyzed from a variety of sources such as corporate America, educational insititutions, and small communities all across the country. However, although the majority of the startups known today are those that found success in climbing the barrier between idea and impact, it is the majority that fall short of overcoming this hurdle and end up failing as a product. In this report, we aim to look at a multitude of variables directly and intrinsically tied to startups and their growth, to determine the coefficient of correlation between various factors such as industry sector, funding, location, and size, and how they impact a startup's ability to come to fruition. Because the growth of startups is relatively new and tied to very modern technological advancements, there is a scarce amount of research done into the causes behind their success and failures. For example, venture capital firms and startup accelerators such as Y Combinator were founded in 2005, making the funding rounds for successful startups a very new principle. Our curiosity lies in looking at the underlying details of the successes and failures for startups in the United States, as it provides an opportunity to discover findings in a modern niche that does not possess the level of academic study that other corporate fields in America do."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Startups with larger funding sizes, operating in high-growth industry sectors, and located within established entrepreneurial ecosystems are less likely to fail and tend to experience longer survival times before either failure or acquisition, whereas startups with smaller funding, in low-growth sectors, or in emerging regions face higher failure risks and shorter time-to-event durations.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "- **Dataset Name**: Startup Success Prediction\n",
    "  - **Link to the dataset**: 'https://www.kaggle.com/datasets/manishkc06/startup-success-prediction/data'\n",
    "  - **Number of observations**: 922 \n",
    "  - **Number of variables**: 49 \n",
    "  - **Most relevant variables for our project**:\n",
    "    - Name/ID: name (entity key used for de-duplication)\n",
    "    - Industry/Sector: labels (needs standardization to top-level sector groups)\n",
    "    - Geography: e.g., city, state_code (normalize to country/region for stratification)\n",
    "    - Funding: e.g., funding_total_usd, first_funding_at, last_funding_at, funding_rounds (USD; heavy-tailed; consider log/winsorization)\n",
    "    - Outcome/Status: status (map to success/failure/censored)\n",
    "    - Timing: founded_at, closed_at (parsed to datetime for time to event metrics)\n",
    "  - **Shortcomings / caveats**:\n",
    "    - Sector taxonomy in labels is noisy; requires collapsing to a consistent hierarchy\n",
    "    - Coverage/selection bias (by region/state/stage) likely; report as limitation and consider cohort/time controls\n",
    "    - Event labeling: status may lag reality; treat \"operating\" as censored in survival framing\n",
    "    - Date gaps: Missing founded_at/closed_at can bias time-to-event; document drop/impute rules\n",
    "\n",
    "- **Dataset #2**\n",
    "  - **Dataset Name**: Big Startup Success/Fail Dataset from Crunchbase\n",
    "  - **Link to the dataset**: `https://www.kaggle.com/datasets/yanmaksi/big-startup-secsees-fail-dataset-from-crunchbase`\n",
    "  - **Number of observations**: Computed in the code cell below (after loading)\n",
    "  - **Number of variables**: Computed in the code cell below (after loading)\n",
    "  - **Most relevant variables for our project**:\n",
    "    - Funding-related: `funding_total_usd` (or similar), `funding_rounds`, `last_funding_at`\n",
    "    - Outcome/status: `status` (e.g., acquired/ipo/operating/closed), `acquired_at`, `closed_at`\n",
    "    - Company profile: `name`, `category`/`industry`, `country`, `region`/`city`, `num_employees`\n",
    "    - Timing: `founded_at` (and event dates as above) for computing time-to-failure or time-to-acquisition\n",
    "  - **Shortcomings / caveats**:\n",
    "    - Potential label noise: outcomes may be simplified; “operating” does not guarantee long-term success; “closed” labels may lag reality\n",
    "    - Missing or inconsistent dates across companies, which affects survival/time-to-event analyses\n",
    "    - Possible duplicates or multiple rows for the same company over time; category/industry taxonomy can be messy\n",
    "    - Survivorship and reporting bias inherent to Crunchbase; geographic and sector coverage is uneven\n",
    "\n",
    "If we combine datasets, we will align on common keys (e.g., standardized company names and/or website domains), and normalize geography (country/region) and industry taxonomies before joining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Download Progress:  50%|█████     | 1/2 [00:00<00:00,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: airline-safety.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Download Progress: 100%|██████████| 2/2 [00:00<00:00,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: bad-drivers.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { 'url': 'https://drive.google.com/file/d/1rmMvNVkUbmVFAGQL12egPtsi1KxcM1GA/view?usp=sharing', 'filename':'dataset1.csv'},\n",
    "    { 'url': 'https://drive.google.com/file/d/1QoszX-uQ2K_TnbDIrMpAgUEyVi9QHF9v/view?usp=sharing', 'filename':'dataset2.csv'}\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
  "attachments": {},
  "cell_type": "markdown",
  "metadata": {},
  "source": [
    "### Dataset #1 — Startup Success Prediction (Kaggle)\n",
    "\n",
    "This dataset provides company-level attributes (name, sector labels, geography, funding history, outcome/status, and key dates) suitable for modeling both likelihood of outcomes and timing analysis. It complements Dataset #2 and allows cross-validation of patterns at smaller scale with richer label coverage.\n",
    "\n",
    "Instructions:\n",
    "1) Load and standardize\n",
    "- Load `data/00-raw/dataset1.csv` with pandas.\n",
    "- Standardize column names to snake_case: trim, replace spaces/hyphens/slashes/periods, lowercase.\n",
    "- Print shape and a short description of rows/columns.\n",
    "\n",
    "2) Missingness profiling\n",
    "- Compute percent missing per column; list top 10.\n",
    "- Compute a simple missingness correlation matrix to surface possible systematic gaps; print top correlated pairs.\n",
    "\n",
    "3) Outliers and types\n",
    "- If present, coerce `funding_total_usd` to numeric (strip commas/`$` if needed).\n",
    "- Detect outliers via IQR and create `funding_outlier_flag`.\n",
    "\n",
    "4) Minimal cleaning and tidying\n",
    "- Drop exact duplicate rows; then drop duplicates by entity key if appropriate.\n",
    "- Drop rows where both `name` and `status` are missing (if they exist).\n",
    "- Parse date-like fields when present (e.g., `founded_at`, `closed_at`).\n",
    "\n",
    "5) Summary statistics and export\n",
    "- Print summary stats for key variables: `funding_total_usd`, `funding_rounds`, `status`, `city`, `state_code` (only those that exist).\n",
    "- Save the cleaned file to `data/02-processed/dataset1_startup_data_processed.csv`.\n",
    "\n",
    "Caveats:\n",
    "- Sector labels are noisy; collapse to a consistent high-level taxonomy if used downstream.\n",
    "- Funding is heavy‑tailed; consider log1p or winsorization in modeling.\n",
    "- Event/status labels can lag reality; treat \"operating\" as censored in survival analyses.\n",
    "- Missing dates can bias time-to-event; document and justify imputation/drop rules.\n"
  ]
},
{
  "cell_type": "code",
  "execution_count": null,
  "metadata": {},
  "outputs": [],
  "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n"
  ]
},
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #2 — Crunchbase Startup Outcomes (Failure vs. Acquisition)\n",
    "\n",
    "This dataset contains company-level records from Crunchbase with profile attributes (name, industry/category), geography (country, region/city), funding history (total USD raised, number of rounds, last funding date), and outcome indicators (operating, acquired, IPO, closed) with event timestamps. It directly supports our hypothesis by enabling both:\n",
    "\n",
    "- Likelihood modeling: whether a startup ultimately fails (closed) or is successfully exited (acquired/IPO).\n",
    "- Timing modeling: how long it takes from founding to failure vs. acquisition (time-to-event).\n",
    "\n",
    "Scope and variables aligned to the hypothesis:\n",
    "- **Funding size and intensity**: `funding_total_usd` (USD, heavy‑tailed), `funding_rounds` (count), `last_funding_at` (recency control).\n",
    "- **Industry sector**: `category`/`industry`/`category_list` (will standardize to a consistent taxonomy and allow multi-label handling when needed).\n",
    "- **Geographic location**: `country`/`country_code`, `region`/`city` (we will normalize to country and, when available, region for stratification).\n",
    "- **Outcomes and timing**: `status` plus event dates `acquired_at`, `closed_at`, and baseline `founded_at` to compute durations.\n",
    "\n",
    "Outcome coding and analysis framing:\n",
    "- **Success**: acquired or IPO.\n",
    "- **Failure**: closed.\n",
    "- **Censored**: operating/active at last observation (no terminal event yet).\n",
    "- We will treat timing via survival/competing‑risks framing: time from `founded_at` to first terminal event (acquisition or closure), censoring otherwise. This supports estimating how funding, sector, and geography shift both hazards and ultimate probabilities.\n",
    "\n",
    "Key data characteristics and handling notes:\n",
    "- **Heavy tails in funding**: consider log1p transform or winsorization; flag extreme outliers for sensitivity checks.\n",
    "- **Sector taxonomy**: categories are noisy/inconsistent; we will collapse to a manageable hierarchy (e.g., top‑level sector groups).\n",
    "- **Geographic normalization**: standardize country codes; map regions/cities to country, and use region only when coverage is sufficient.\n",
    "- **Missingness**: dates and categories can be absent; we will profile missingness, document imputation/dropping rules, and avoid introducing informative censoring.\n",
    "- **Entity resolution**: de‑duplicate by exact rows and harmonized `name`; spot‑check ambiguous entities.\n",
    "- **Coverage bias**: Crunchbase coverage varies by sector, geography, and stage; we will include cohort/time fixed effects where needed and report limitations.\n",
    "\n",
    "Wrangling plan(TBD, can be altered...):\n",
    "- Load from `data/00-raw/`, standardize column names and types, parse dates, harmonize sector and geography, derive `outcome_label` and `time_to_event_days`, flag funding outliers, and export an analysis‑ready file to `data/02-processed/` for modeling likelihood and timing of failure vs. acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected raw file: data/00-raw/dataset2.csv\n",
      "Raw shape: (66368, 14)\n",
      "Dropped duplicates -> 66368 -> 66103 rows\n",
      "Working shape (selected columns): (66103, 9)\n",
      "\n",
      "Missingness (% of rows missing per column):\n",
      "event_date            100.0\n",
      "time_to_event_days    100.0\n",
      "founded_at             22.9\n",
      "funding_total_usd      19.2\n",
      "region                 12.1\n",
      "country_code           10.5\n",
      "category_list           4.7\n",
      "name                    0.0\n",
      "status                  0.0\n",
      "funding_rounds          0.0\n",
      "last_funding_at         0.0\n",
      "outcome_label           0.0\n",
      "dtype: float64\n",
      "Funding IQR bounds -> lower=0, upper=24,495,250; outliers flagged: 7363\n",
      "Dropped rows missing all of ['name', 'status']: 66103 -> 66103\n",
      "\n",
      "Processed file written to: data/02-processed/startups_crunchbase_processed.csv\n",
      "\n",
      "Summary:\n",
      "{'n_rows': 66103, 'n_cols': 13, 'outcome_counts': {'censored': 52812, 'success': 7088, 'failure': 6203}}\n"
     ]
    }
   ],
   "source": [
    "# Crunchbase Startup Success/Fail — Load, Clean, Tidy, Wrangle\n",
    "#\n",
    "# Notes:\n",
    "# - Place the Kaggle CSV from the dataset into `data/00-raw/`.\n",
    "# - If multiple CSVs exist, this cell attempts to auto-detect the most likely file.\n",
    "# - Adjust `RAW_FILENAME_OVERRIDE` if auto-detection fails.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "RAW_DIR = Path(\"data/00-raw\")\n",
    "INTERIM_DIR = Path(\"data/01-interim\")\n",
    "PROCESSED_DIR = Path(\"data/02-processed\")\n",
    "\n",
    "INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# If you know the exact filename, set it here; otherwise keep as None\n",
    "RAW_FILENAME_OVERRIDE: Optional[str] = \"dataset2.csv\"  # renamed for consistency\n",
    "\n",
    "# Heuristics for picking likely files\n",
    "LIKELY_SUBSTRINGS: List[str] = [\n",
    "    \"crunchbase\",\n",
    "    \"startup\",\n",
    "    \"startups\",\n",
    "    \"success\",\n",
    "    \"fail\",\n",
    "]\n",
    "\n",
    "\n",
    "def pick_raw_file(raw_dir: Path, override: Optional[str]) -> Path:\n",
    "    \"\"\"Pick the most likely raw CSV file for the Crunchbase dataset.\n",
    "\n",
    "    Args:\n",
    "        raw_dir: Directory containing raw files.\n",
    "        override: Explicit filename if known.\n",
    "\n",
    "    Returns:\n",
    "        Path to the selected CSV file.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If no suitable file is found.\n",
    "    \"\"\"\n",
    "    if override is not None:\n",
    "        candidate = raw_dir / override\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "        raise FileNotFoundError(\n",
    "            f\"Override file not found: {candidate}. Place it in {raw_dir} or update RAW_FILENAME_OVERRIDE.\"\n",
    "        )\n",
    "\n",
    "    csvs = sorted(raw_dir.glob(\"*.csv\"))\n",
    "    if not csvs:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No CSV files found in {raw_dir}. Download the Kaggle dataset and place the CSV here.\"\n",
    "        )\n",
    "\n",
    "    # Rank by substring match then by size (descending)\n",
    "    def score(p: Path) -> tuple[int, int]:\n",
    "        name = p.name.lower()\n",
    "        match_score = sum(1 for s in LIKELY_SUBSTRINGS if s in name)\n",
    "        size_score = p.stat().st_size\n",
    "        return (match_score, size_score)\n",
    "\n",
    "    ranked = sorted(csvs, key=score, reverse=True)\n",
    "    return ranked[0]\n",
    "\n",
    "\n",
    "raw_path = pick_raw_file(RAW_DIR, RAW_FILENAME_OVERRIDE)\n",
    "print(f\"Selected raw file: {raw_path}\")\n",
    "\n",
    "# Load data\n",
    "# Use low_memory=False for mixed types, and keep original column names for inspection before standardization\n",
    "raw_df = pd.read_csv(raw_path, low_memory=False)\n",
    "print(f\"Raw shape: {raw_df.shape}\")\n",
    "\n",
    "# Standardize column names: snake_case\n",
    "raw_df.columns = (\n",
    "    raw_df.columns.str.strip().str.replace(\" \", \"_\", regex=False).str.replace(\"-\", \"_\", regex=False).str.lower()\n",
    ")\n",
    "\n",
    "# Basic de-duplication by exact row match and by name if present\n",
    "if \"name\" in raw_df.columns:\n",
    "    before = len(raw_df)\n",
    "    raw_df = raw_df.drop_duplicates()\n",
    "    raw_df = raw_df.drop_duplicates(subset=[\"name\"], keep=\"first\")\n",
    "    print(f\"Dropped duplicates -> {before} -> {len(raw_df)} rows\")\n",
    "else:\n",
    "    raw_df = raw_df.drop_duplicates()\n",
    "\n",
    "# Identify key columns by fuzzy names\n",
    "def find_col(candidates: List[str]) -> Optional[str]:\n",
    "    cols = set(raw_df.columns)\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "col_name = find_col([\"name\", \"company\", \"company_name\"])\n",
    "col_status = find_col([\"status\", \"state\", \"current_status\"])  # typical outcome label\n",
    "col_funded_total = find_col([\"funding_total_usd\", \"funding_total\", \"total_funding_usd\", \"total_funding\"])\n",
    "col_rounds = find_col([\"funding_rounds\", \"num_funding_rounds\", \"rounds\"])\n",
    "col_category = find_col([\"category\", \"category_list\", \"industry\", \"category_groups\"])\n",
    "col_country = find_col([\"country\", \"country_code\", \"country_iso\", \"country_name\"])\n",
    "col_region = find_col([\"region\", \"state_region\", \"state\", \"city\", \"location\"])\n",
    "col_founded = find_col([\"founded_at\", \"founded\", \"founded_date\"])  # date-like\n",
    "col_acquired = find_col([\"acquired_at\", \"acquired\", \"acquisition_date\"])  # date-like\n",
    "col_closed = find_col([\"closed_at\", \"closed\", \"closing_date\"])  # date-like\n",
    "col_last_funding = find_col([\"last_funding_at\", \"last_funding_date\"])  # date-like\n",
    "col_employees = find_col([\"num_employees\", \"employee_count\", \"employees\"])\n",
    "\n",
    "# Restrict to relevant columns (retain existing only)\n",
    "keep_cols: List[str] = [\n",
    "    c\n",
    "    for c in [\n",
    "        col_name,\n",
    "        col_status,\n",
    "        col_funded_total,\n",
    "        col_rounds,\n",
    "        col_category,\n",
    "        col_country,\n",
    "        col_region,\n",
    "        col_founded,\n",
    "        col_acquired,\n",
    "        col_closed,\n",
    "        col_last_funding,\n",
    "        col_employees,\n",
    "    ]\n",
    "    if c is not None\n",
    "]\n",
    "\n",
    "work_df = raw_df[keep_cols].copy()\n",
    "print(f\"Working shape (selected columns): {work_df.shape}\")\n",
    "\n",
    "# Parse dates where present\n",
    "for dcol in [col_founded, col_acquired, col_closed, col_last_funding]:\n",
    "    if dcol is not None and dcol in work_df.columns:\n",
    "        work_df[dcol] = pd.to_datetime(work_df[dcol], errors=\"coerce\")\n",
    "\n",
    "# Coerce funding to numeric USD if present\n",
    "if col_funded_total is not None and col_funded_total in work_df.columns:\n",
    "    # Remove currency symbols/commas if any\n",
    "    work_df[col_funded_total] = (\n",
    "        work_df[col_funded_total]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \"\", regex=False)\n",
    "        .str.replace(\"$\", \"\", regex=False)\n",
    "        .str.strip()\n",
    "    )\n",
    "    work_df[col_funded_total] = pd.to_numeric(work_df[col_funded_total], errors=\"coerce\")\n",
    "\n",
    "# Derive outcome label: success/failure/censored\n",
    "# success: acquired or IPO; failure: closed; censored: operating/others with no terminal date\n",
    "def derive_outcome(status_val: Optional[str]) -> Optional[str]:\n",
    "    if status_val is None or pd.isna(status_val):\n",
    "        return None\n",
    "    s = str(status_val).strip().lower()\n",
    "    if any(tok in s for tok in [\"acquired\", \"ipo\"]):\n",
    "        return \"success\"\n",
    "    if \"closed\" in s:\n",
    "        return \"failure\"\n",
    "    if any(tok in s for tok in [\"operating\", \"active\"]):\n",
    "        return \"censored\"\n",
    "    return None\n",
    "\n",
    "if col_status is not None and col_status in work_df.columns:\n",
    "    work_df[\"outcome_label\"] = work_df[col_status].map(derive_outcome)\n",
    "else:\n",
    "    work_df[\"outcome_label\"] = None\n",
    "\n",
    "# Time-to-event from founded to first terminal event if present\n",
    "def pick_event_date(row: pd.Series) -> pd.Timestamp | pd.NaT:\n",
    "    # prefer acquired/closed dates; else NaT\n",
    "    acquired_date = row[col_acquired] if col_acquired in row and pd.notna(row[col_acquired]) else pd.NaT\n",
    "    closed_date = row[col_closed] if col_closed in row and pd.notna(row[col_closed]) else pd.NaT\n",
    "    # pick whichever is earliest non-NaT\n",
    "    if pd.notna(acquired_date) and pd.notna(closed_date):\n",
    "        return min(acquired_date, closed_date)\n",
    "    if pd.notna(acquired_date):\n",
    "        return acquired_date\n",
    "    if pd.notna(closed_date):\n",
    "        return closed_date\n",
    "    return pd.NaT\n",
    "\n",
    "if col_founded is not None and col_founded in work_df.columns:\n",
    "    work_df[\"event_date\"] = work_df.apply(pick_event_date, axis=1)\n",
    "    work_df[\"time_to_event_days\"] = (\n",
    "        work_df[\"event_date\"] - work_df[col_founded]\n",
    "    ).dt.days\n",
    "else:\n",
    "    work_df[\"event_date\"] = pd.NaT\n",
    "    work_df[\"time_to_event_days\"] = np.nan\n",
    "\n",
    "# Missingness profile\n",
    "missing_pct = work_df.isna().mean().sort_values(ascending=False)\n",
    "print(\"\\nMissingness (% of rows missing per column):\")\n",
    "print((missing_pct * 100).round(1))\n",
    "\n",
    "# Flag outliers in funding (IQR method)\n",
    "if col_funded_total is not None and col_funded_total in work_df.columns:\n",
    "    fund = work_df[col_funded_total].dropna()\n",
    "    if len(fund) > 0:\n",
    "        q1, q3 = np.percentile(fund, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        upper = q3 + 1.5 * iqr\n",
    "        lower = max(0, q1 - 1.5 * iqr)\n",
    "        work_df[\"funding_outlier_flag\"] = (\n",
    "            (work_df[col_funded_total] < lower) | (work_df[col_funded_total] > upper)\n",
    "        )\n",
    "        print(\n",
    "            f\"Funding IQR bounds -> lower={lower:,.0f}, upper={upper:,.0f}; \"\n",
    "            f\"outliers flagged: {work_df['funding_outlier_flag'].sum()}\"\n",
    "        )\n",
    "    else:\n",
    "        work_df[\"funding_outlier_flag\"] = False\n",
    "else:\n",
    "    work_df[\"funding_outlier_flag\"] = False\n",
    "\n",
    "# Minimal cleaning: drop rows with no name and no status\n",
    "min_keep_cols = [c for c in [col_name, col_status] if c is not None]\n",
    "if min_keep_cols:\n",
    "    before = len(work_df)\n",
    "    work_df = work_df.dropna(subset=min_keep_cols, how=\"all\")\n",
    "    print(f\"Dropped rows missing all of {min_keep_cols}: {before} -> {len(work_df)}\")\n",
    "\n",
    "# Save processed\n",
    "processed_path = PROCESSED_DIR / \"startups_crunchbase_processed.csv\"\n",
    "work_df.to_csv(processed_path, index=False)\n",
    "print(f\"\\nProcessed file written to: {processed_path}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nSummary:\")\n",
    "print({\n",
    "    \"n_rows\": len(work_df),\n",
    "    \"n_cols\": work_df.shape[1],\n",
    "    \"outcome_counts\": work_df[\"outcome_label\"].value_counts(dropna=False).to_dict() if \"outcome_label\" in work_df else {},\n",
    "})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Bias & fairness:** Datasets may favor startups or sectors that are much higher in popularity and show a bias towards well known startups. There are also concerns with issues of generlzation as some areas may have high density of startups compared to others potential misrepresenting the data.\n",
    "- **Generalization limits:** As mentioned previously, Kaggle datasets in particular may overrepresent high success startups due to the ease of accessing the data. This results in the data not geenrlazaing to non-tech or smaller companies. We intend to avoid general claims and make specifc statements that are contextualized by environment maturity.\n",
    "- **Data sensitivity:** Although the data gathered is public, names and emails can be used to reidenifty an individual, or any specific data points that can be used to triangulate a person or startup. \n",
    "- **Non-Consensual Use of Company Information:** Despite the data being public, startups in the dataset did not consent to be analyzed or used for prediction exercises. We will be using aggregated analysis and not single out any companies.\n",
    "- **Potential Misrepresentation Due to Inaccurate or Incomplete Data:** Startup databases are often incomplete, outdated, or wrong because the data is crowdsourced. We will treat the data as approximate and emphasize uncertainty rather than presenting results as definitive truth.\n",
    "- **Responsibility to Prevent Harmful Use of Results:** If someone misuses the findings it may influence funding or hiring decisions, or perceptions of certain industries/regions. We will explicitly state that the work should not be used for investment decisions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regards to communication, we plan on using iMessage to text one another. We believe this is one of the most efficient and easiest ways for us to contact one another. As for the ,essaging itself, all members have agreed to expect a rresponse, whether that be a message or a reaction, from each group member within 3 hours of the initial text being sent. We will meet twice per week, every Monday morning we will reserve a study space in the Geisel Library to meet and prodvide updates, followed by a Zoom meetimng every Friday afternoon to consolidate the designated progress from Onday's meeting. In regarcs to tone, we agree to all expect respectful interactions. Even when disagreement takes place, the person expressing the lack of approval should explain their reasonibg, as well as an alkternative method that they beleive is better. We will be concise and to the point, but still maintaining respect for one another and everyone's ideas. We plan to use voting to make decisions as a group, especially for disagreements and changes to our original plan. The project administrator will be in charge of calling teh vote, ad we will go with the majority ruling, as wel have3 5 people. We will not accept abstaining from votes. We do have specialized roles for each person, hwoever, because there is overlap, we do plan to share a lot of the responsibilties. As we are a team, we plan on heloing each other out when possible, especially if one person is struggling with a specific task. We assign roles and tasks based on the skillsets of the members, which we have laready discussed in detail. We have set a policy that struggles are inevitbale, as we are all busy. We have a guideline that whenever someone is falling behind, there is no hassle or problem in expressing that as early as possible. We would rather know what to fix earlier on in the process, rather than have a last minute lack of execution. Struggles with certain tasks hoild be expressd immediately, as we will set egos aside ti help regardless of role/assignned tasks, in roder to priotitize the team as a unit/whole.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline\n",
    "### Week 3\n",
    "- **Monday:** Meeting to brainstorm project topics, confirm individual dataset search responsibilities and initiate the plan for our topic.\n",
    "- **Friday:** Zoom meeting to vote on and finalize topic.,\n",
    "- **Sunday:** Zoom meeting to discuss roles then consolidate and review the datasets we’ve individually found.\n",
    "\n",
    "### Week 4\n",
    "- **Monday:** Meet in Geisel to consolidate datasets and assign roles for our project proposal. Begin working on data cleaning plan, early transformations, and outline visualization goals.\n",
    "- **Wednesday:** Proofread and finalize proposal\n",
    "\n",
    "\n",
    "### Week 5,\n",
    "- **Monday:** Begin data cleaning and preprocessing our datasets.\n",
    "- **Wednesday:** Continue data cleaning and finalize our structured and processed dataset; share cleaned files with each other.\n",
    "\n",
    "### Week 6\n",
    "- **Sunday:** Zoom meeting to compare our processed datasets and make sure everything is consistent.\n",
    "- **Monday:** Discuss trends and finalize consensus on dataset selection and structure.\n",
    "- **Wednesday:** Complete initial EDA preparation, finalize plan for visualization types.\n",
    "\n",
    "### Week 7\n",
    "- **Monday:** Start building visualizations, assign figure responsibilities to group members.\n",
    "- **Friday:** Review meeting to make sure visualizations are progressing and discuss results and narrative.\n",
    "\n",
    "### Week 8\n",
    "- **Monday:** Compile all visualizations and ensure consistency.\n",
    "- **Wednesday:** Polish everything, ensure code runs cleanly.\n",
    "\n",
    "### Week 9\n",
    "- **Monday:** Final review session, proofread notebook text, verify rubric requirements, and finalize project. Begin preparing video.\n",
    "- **Wednesday:** Submit final project and recording; complete team evaluation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
